# BInarized Transformers for Making fAst PredictionS
Transformer neural networks have revolutionized NLP in a wide variety of tasks including language modeling, machine translation, and named entity recognition. 
Despite their usefulness, transformers are difficult to run on edge devices because of their hefty computing requirements. 

Previously, binarization techniques, which involve reducing the precision of a model's parameters from float-32 to a single bit,
have worked to great success in running convolutional neural networks on mobile phones. However, there do not currently such methods for transformers.
We aim to binarize transformers in order to run them efficiently on edge devices. 

# Masked Language Modeling with BERT
...

# Binarization Strategies
...
